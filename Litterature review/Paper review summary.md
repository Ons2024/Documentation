


| Title                                                                                                                   | Journal / Conference |     | Keywords                    | Purpose                                                                  | Architecture / Method                  | Dataset Used                            | Key Findings                                                   | Key Takeaway                                             | Limitations                             |
| ----------------------------------------------------------------------------------------------------------------------- | -------------------- | --- | --------------------------- | ------------------------------------------------------------------------ | -------------------------------------- | --------------------------------------- | -------------------------------------------------------------- | -------------------------------------------------------- | --------------------------------------- |
| <br>_A scalable learning approach for user equilibrium traffic assignment problem using graph convolutional networks_\| | NeurIPS 2017         |     | Transformer, NLP, Attention | To propose a novel architecture for sequence modeling without recurrence | Transformer (self-attention mechanism) | WMT 2014 English-German, English-French | Outperformed RNNs on translation tasks with less training time | Introduced self-attention, enabling parallel computation | Limited evaluation on non-text datasets |
|                                                                                                                         |                      |     |                             |                                                                          |                                        |                                         |                                                                |                                                          |                                         |
|                                                                                                                         |                      |     |                             |                                                                          |                                        |                                         |                                                                |                                                          |                                         |
|                                                                                                                         |                      |     |                             |                                                                          |                                        |                                         |                                                                |                                                          |                                         |
