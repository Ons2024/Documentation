


| Title                                | Journal / Conference | Keywords                    | Purpose                                                                  | Architecture / Method                  | Dataset Used                            | Key Findings                                                   | Key Takeaway                                             | Limitations                             |
| ------------------------------------ | -------------------- | --------------------------- | ------------------------------------------------------------------------ | -------------------------------------- | --------------------------------------- | -------------------------------------------------------------- | -------------------------------------------------------- | --------------------------------------- |
| Example: "Attention Is All You Need" | NeurIPS 2017         | Transformer, NLP, Attention | To propose a novel architecture for sequence modeling without recurrence | Transformer (self-attention mechanism) | WMT 2014 English-German, English-French | Outperformed RNNs on translation tasks with less training time | Introduced self-attention, enabling parallel computation | Limited evaluation on non-text datasets |
|                                      |                      |                             |                                                                          |                                        |                                         |                                                                |                                                          |                                         |
|                                      |                      |                             |                                                                          |                                        |                                         |                                                                |                                                          |                                         |
