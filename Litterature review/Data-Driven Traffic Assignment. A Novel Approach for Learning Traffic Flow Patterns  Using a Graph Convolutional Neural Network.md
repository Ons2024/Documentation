
## Core Idea

This study introduces a **data-driven approach** to learn **traffic flow patterns** in transportation networks directly from data, ==without assuming user behavior models like user equilibrium or system optimal==.
The method reformulates the traditional **traffic assignment problem** as a **learning task**, using a **Graph Convolutional Neural Network (GCNN)** to model how travel demand from various origins and destinations diffuses through network links.

## Problematic

The **Traffic Assignment Problem (TAP)** is central to transportation planning, traditionally solved using **User Equilibrium (UE)** models that assume perfect driver knowledge and rational route choices. However, these assumptions often fail in real-world, dynamic traffic conditions.

While **Dynamic Traffic Assignment (DTA)** models offer more realism, they are computationally expensive and unsuitable for large-scale, real-time applications.


With the rise of **traffic sensors**, **connected vehicles**, and **big data**, there is growing potential for **data-driven approaches** to estimate and forecast traffic flows. Yet, most existing data-driven models only handle **short-term predictions** on small road segments and fail to capture **network-wide dynamics** or **travel demand variations**.

To address these gaps, this study introduces a **Graph Convolutional Neural Network (GCNN)** framework that learns ***traffic flow patterns** directly from data*â€”without relying on behavioral assumptions. By representing transportation networks as graphs and modeling the **diffusion of origin-destination (OD) demand** through links.

In this work, they seek to answer the question:
> ==Can a deep learning model learn the flow patterns of a network without relying on the assumptions of user behavior for assigning traffic in the network?==


## DATA-DRIVEN TRAFFIC ASSIGNMENT


- **Transportation network** is represented as a **weighted directed graph**.
- Each link between nodes carries information such as **distance, speed limit, capacity,** and **free-flow travel time**.
- The goal is to learn a **mapping function** that relates **origin-destination (OD) travel demands** to **link flows** in the network using available data and network characteristics (e.g., node locations, travel times).


### Transportation Network Model

The transportation network is modeled as a **weighted directed graph**:

$$
\mathcal{G}(v, \mathcal{E}, A_w)
$$

where:

- $v$: set of **nodes**
- $\mathcal{E}$: set of **links** $(i \rightarrow j)$
- $A_w$: **weighted adjacency matrix** based on free-flow travel time

#### Adjacency Weights

It anserws this question : â€œHow long does it take to go from node _i_ to node _j_ when thereâ€™s no traffic (free-flow condition)?
		![[Pasted image 20251018115741.png]]

- **$t^0$** denotes the free flow travel time between the origin and the destination nodes

---
#### Inputs

- **Origin-Destination (OD) demand matrices:**  
  $$
  [X_1, X_2, ..., X_m]
  $$

---
#### Outputs

- **Corresponding ==link flow== vectors:**  
  $$
  [F_1, F_2, ..., F_m]
  $$
---
#### Objective

Learn a **mapping function**:

$$
\mathcal{F}([X_1, X_2, ..., X_m]; \mathcal{G}(v, \mathcal{E}, A_w)) = [F_1, F_2, ..., F_m]
$$

=> that predicts **link flows** from **OD demands** and **network structure**.

The model captures **flow propagation** from origins to destinations within the network.


![[Pasted image 20251018120503.png]]






### Graph Convolution Neural Network for Flow Pattern Learning

1. **Convolutional Neural Networks (CNNs)**

	##### **The main idea**
	    
	Instead of looking at all the input data at once (like a regular neural network), a CNN looks at **small parts** (or **patches**) of the data using a **filter** (also called a **kernel**).
		
	These filters â€œslideâ€ across the input and detect patterns such as:
	- Edges
	- Shapes
	- More complex features as the network gets deeper
			
	This operation is called **convolution**
	
	---
	
	### **CNN architecture â€” step by step**
	
	#### **(a) Input Layer**
	
	- Example: an image of size 28Ã—28 pixels (like grayscale handwriting).
	    
	
	#### **(b) Convolution Layer**
	
	- Applies **filters** (like small 3Ã—3 or 5Ã—5 grids).
	    
	- Each filter extracts specific **spatial features** (like edges or corners).
	    
	- Output: a new set of â€œfeature mapsâ€ that highlight important parts of the image.
	    
	
	#### **(c) Activation Function**
	
	- Usually **ReLU** (Rectified Linear Unit): keeps positive signals, removes negatives.
	    
	- Adds **non-linearity** â€” helps model complex relationships.
	    
	
	#### **(d) Pooling Layer**
	
	- Reduces the size of the data (by taking averages or maximum values).
	    
	- Helps the network **focus on the most important features** and speeds up training.
	    
	
	#### **(e) Fully Connected (Dense) Layer**
	
	- Combines all extracted features to make a **final prediction** â€” e.g.,  
	â€œThis is a carâ€ ğŸš— or â€œThis is a catâ€ ğŸ±.
	
	---
	 **Limitation**
	CNNs work great on grids (like images) but not on networks (graphs), because:  
	Roads and intersections are irregularly connected â€” not arranged in neat rows and columns.  
	So, a standard CNN canâ€™t properly capture how traffic flows through complex road networks.

==CNNs treat the network as an image â†’ fail to capture actual **traffic flow relationships** among nodes and links.==

2. **Graph Convolutional Neural Network (GCNN)**

    The GCNN replaces the image-based convolution with **graph convolution**
     *Remember*
     - Each node updates its state by **aggregating information from connected nodes**.
	- This is often called **â€œmessage passing.â€**
	- Over several layers, each node learns not just its own features, but also whatâ€™s happening nearby.
	- The **adjacency matrix (Aw)** tells the model **which nodes are connected** and **how strongly** (e.g., distance, travel time, road capacity).
	- Graph convolution uses this to **control how information flows** through the network â€” mimicking real traffic propagation.

3.  **Modele Architecture** 
	- The GCNN learns **how traffic (flow)** diffuses from **origin nodes to destination nodes** by using **graph convolution**, which considers the networkâ€™s **structure** (node positions, connections, and link attributes).
	
	- The model captures **how flows on one link are influenced by adjacent links**, effectively modeling **traffic propagation** within the network.
	
	- Each node and link are embedded with information (features), and the GCNN jointly learns these **features and relationships**.
	
	- Once the GCNN has learned the **flow diffusion patterns**, the learned representations are passed into a **feed-forward neural network** to estimate the **link traffic flows**.
	
	![[Pasted image 20251018124333.png]]
	
	**Layer 1:** Learns how demand spreads between nodes (network structure).  
	**Layer 2:** Learns how that demand turns into flows along links.  
	**Layer 3:** Aggregates all flows per link to get final traffic flow estimates.
	- The **graph structure (weighted adjacency matrix)** ensures that the model understands how roads are connected.
	- The **parameters (Î˜, Wq, WF)** are learned during training to best map OD demand to observed link flows.


	##### **The main idea: Graph-based Flow Modeling**
	
	âœ”ï¸ They start with the **OD demand matrix** ($X$) and the **adjacency matrix** ($A_w$) that encodes how nodes are connected.  
	âœ”ï¸ They apply a **graph convolution filter** ($g_\theta$) to model how demand spreads across connected nodes.  
	âœ”ï¸ The parameters **Î¸** (Theta) are **learnable** â€” the model adjusts them to best fit observed flow data.  
	âœ”ï¸ They use a **nonlinear activation function** $f_1$ (tanh) to capture complex relationships after the convolution.  
	âœ”ï¸ The output of that first step ($H_1$) goes into the **second layer**, which uses learnable parameters $W_q$ and another activation $f_2$ (tanh).  
	âœ”ï¸ This second layer learns how **demand from each node turns into flows on adjacent links**, giving $q$ (the distributed link flow matrix).  
	âœ”ï¸ Then $q$ is **transposed** ($q^T$) and passed into the **third layer**, which uses $W_F$ and a **linear activation** $f_3$ to produce final **link flows** ($F$).





